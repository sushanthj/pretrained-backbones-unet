{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet with Resnet 50 Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Dataset Size: \n",
    "\n",
    "Val Dataset Size: \n",
    "\n",
    "Test Dataset Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch -q\n",
    "# %pip install opencv-python -q\n",
    "# %pip install pycocotools -q\n",
    "# %pip install timm==0.6.12 -q\n",
    "# %pip install ipdb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sush/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sush/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from backbones_unet.model.unet import Unet\n",
    "from backbones_unet.utils.dataset import SemanticSegmentationDataset\n",
    "from backbones_unet.model.losses import DiceLoss\n",
    "from backbones_unet.utils.trainer import Trainer\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from convert_coco_ann_to_mask import convert_coco_to_mask\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Installation\n",
    "random_tensor = torch.rand((1, 3, 64, 64))\n",
    "model = Unet(in_channels=3, num_classes=1) # if no backbone specified, will default to Resnet50\n",
    "print(model.predict(random_tensor))\n",
    "# summary(model, random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add more items here\n",
    "config = {\n",
    "    \"lr\"         : 1e-3,\n",
    "    \"epochs\"     : 100,\n",
    "    \"batch_size\" : 1,  # Increase if your device can handle it\n",
    "    \"num_classes\": 1,\n",
    "    'truncated_normal_mean' : 0,\n",
    "    'truncated_normal_std' : 0.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a torch.utils.data.Dataset/DataLoader\n",
    "annotation_json_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/annotation.json'\n",
    "train_img_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images'\n",
    "train_mask_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/masks'\n",
    "\n",
    "train_img_path_for_ImageFolder_dataloader = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images_with_class/'\n",
    "\n",
    "#! Temporarily using train and val images as same\n",
    "val_img_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images'\n",
    "val_mask_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/masks'\n",
    "\n",
    "test_img_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images'\n",
    "\n",
    "# img_size = (1384, 1032) # = width, height            # currently PtGrey images\n",
    "img_size = (1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Masks from the COCO annotations (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_coco_to_mask(input_json=annotation_json_path, image_folder=train_img_path, output_folder=train_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mean and std of your dataset:\n",
    "def get_mean_and_std_calculated(IMAGE_DATA_DIR):\n",
    "    \"\"\"\n",
    "    NOTE: The ImageFolder dataloader requires the following file structure:\n",
    "\n",
    "    root\n",
    "    |\n",
    "    └── cat (class label)\n",
    "        |\n",
    "        ├──img_2.png\n",
    "        └──img_1.png\n",
    "\n",
    "    \"\"\"\n",
    "    train_dataset = ImageFolder(IMAGE_DATA_DIR, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    # Initialize lists to store channel-wise means and standard deviations\n",
    "    channel_wise_means = [0.0, 0.0, 0.0]\n",
    "    channel_wise_stds = [0.0, 0.0, 0.0]\n",
    "\n",
    "    # Iterate through the training dataset to calculate means and standard deviations\n",
    "    for image, _ in train_dataset:\n",
    "        for i in range(3):  # Assuming RGB images\n",
    "            channel_wise_means[i] += image[i, :, :].mean().item()\n",
    "            channel_wise_stds[i] += image[i, :, :].std().item()\n",
    "\n",
    "    # Calculate the mean and standard deviation for each channel\n",
    "    num_samples = len(train_dataset)\n",
    "    channel_wise_means = [mean / num_samples for mean in channel_wise_means]\n",
    "    channel_wise_stds = [std / num_samples for std in channel_wise_stds]\n",
    "\n",
    "    # Print the mean and standard deviation for each channel\n",
    "    print(\"Mean:\", channel_wise_means)\n",
    "    print(\"Std:\", channel_wise_stds)\n",
    "\n",
    "    return channel_wise_means, channel_wise_stds\n",
    "\n",
    "# means, stds = get_mean_and_std_calculated(train_img_path_for_ImageFolder_dataloader)\n",
    "means = [0.44895144719250346, 0.4951483853617493, 0.4498602793532975]\n",
    "stds = [0.21388493326245522, 0.24571933703763144, 0.22413276759337405]\n",
    "\n",
    "normalize_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=means, std=stds) # always normalize only after tensor conversion\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSegmentationDataset(img_paths=train_img_path, mask_paths=train_mask_path, size=img_size, mode='binary', normalize=normalize_transform, transformations=None)\n",
    "val_dataset = SemanticSegmentationDataset(img_paths=val_img_path, mask_paths=val_mask_path, size=img_size, mode='binary', normalize=normalize_transform, transformations=None)\n",
    "test_dataset = SemanticSegmentationDataset(img_paths=val_img_path, mask_paths=None, size=img_size, normalize=normalize_transform, transformations=None)\n",
    "\n",
    "temp = train_dataset.__getitem__(1)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory  = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = 2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['batch_size'],\n",
    "    shuffle     = False,\n",
    "    drop_last   = False,\n",
    "    num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    # backbone='convnext_base', # backbone network name\n",
    "    backbone='resnet50',\n",
    "    preprocessing=True,\n",
    "    in_channels=3, # input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    num_classes=config[\"num_classes\"],  # output channels (number of classes in your dataset)\n",
    "    encoder_freeze=True,\n",
    "    pretrained=True,\n",
    ")\n",
    "\n",
    "# model = model().to(device)\n",
    "random_tensor = torch.rand((1, 3, 1024, 1024))\n",
    "print(model.predict(random_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wandb credentials\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"49efd84d0e342f343fb91401332234dea4a3ffe2\") #API Key is in your wandb account, under settings (wandb.ai/settings)\n",
    "\n",
    "run = wandb.init(\n",
    "    name = \"UNet_with_resnet_50\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"IDL_Project_Segmentation\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/sush/klab2/Segmentation_Models/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=config['lr'], betas=(0.9, 0.999), weight_decay=0.05)\n",
    "gamma = 0.8\n",
    "milestones = [10,20,40,60,80]\n",
    "\n",
    "# scheduler1 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.9, total_iters=5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "# scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[20, 51])\n",
    "\n",
    "mixed_precision_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,              # UNet model with Resnet50 backbone\n",
    "    criterion=DiceLoss(),     # loss function\n",
    "    optimizer=optimizer,\n",
    "    epochs=10,\n",
    "    scaler=mixed_precision_scaler,\n",
    "    lr_scheduler=scheduler,\n",
    "    device=device,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "\n",
    "trainer.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
