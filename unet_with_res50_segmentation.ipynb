{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet with Resnet 50 Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Dataset Size: \n",
    "\n",
    "Val Dataset Size: \n",
    "\n",
    "Test Dataset Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch -q\n",
    "# %pip install opencv-python -q\n",
    "# %pip install pycocotools -q\n",
    "# %pip install timm==0.6.12 -q\n",
    "# %pip install ipdb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sush/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from backbones_unet.model.unet import Unet\n",
    "from backbones_unet.utils.dataset import SemanticSegmentationDataset\n",
    "from backbones_unet.model.losses import DiceLoss\n",
    "from backbones_unet.utils.trainer import Trainer\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from convert_coco_ann_to_mask import convert_coco_to_mask\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.6458,  3.0477,  1.9521,  ..., -0.9206, -1.2031,  0.3762],\n",
      "          [ 2.4717,  4.4546,  1.0883,  ..., -2.4670, -1.3348,  1.2135],\n",
      "          [ 0.3253,  0.2207, -0.5515,  ..., -0.8310,  1.0622,  1.1272],\n",
      "          ...,\n",
      "          [-0.4217, -0.5321,  0.5751,  ...,  0.5326,  0.6465,  0.6894],\n",
      "          [-1.4970, -2.0495,  0.0494,  ..., -0.8868,  0.3673, -0.1797],\n",
      "          [-0.5489, -1.8801, -0.4927,  ..., -0.0623,  0.8461,  1.2144]]]])\n"
     ]
    }
   ],
   "source": [
    "# Test Installation\n",
    "random_tensor = torch.rand((1, 3, 64, 64))\n",
    "model = Unet(in_channels=3, num_classes=1) # if no backbone specified, will default to Resnet50\n",
    "print(model.predict(random_tensor))\n",
    "# summary(model, random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add more items here\n",
    "config = {\n",
    "    \"lr\"         : 1e-4,\n",
    "    \"epochs\"     : 100,\n",
    "    \"batch_size\" : 1,  # Increase if your device can handle it\n",
    "    \"num_classes\": 1,\n",
    "    'truncated_normal_mean' : 0,\n",
    "    'truncated_normal_std' : 0.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a torch.utils.data.Dataset/DataLoader\n",
    "annotation_json_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/annotation.json'\n",
    "train_img_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images'\n",
    "train_mask_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/masks'\n",
    "\n",
    "train_img_path_for_ImageFolder_dataloader = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images_with_class/'\n",
    "\n",
    "#! Temporarily using train and val images as same\n",
    "val_img_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images'\n",
    "val_mask_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/masks'\n",
    "\n",
    "test_img_path = '/home/sush/klab2/rosbags_collated/sensors_2023-08-03-15-19-03_0/images'\n",
    "\n",
    "# img_size = (1384, 1032) # = width, height            # currently PtGrey images\n",
    "img_size = (1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Masks from the COCO annotations (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_coco_to_mask(input_json=annotation_json_path, image_folder=train_img_path, output_folder=train_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mean and std of your dataset:\n",
    "def get_mean_and_std_calculated(IMAGE_DATA_DIR):\n",
    "    \"\"\"\n",
    "    NOTE: The ImageFolder dataloader requires the following file structure:\n",
    "\n",
    "    root\n",
    "    |\n",
    "    └── cat (class label)\n",
    "        |\n",
    "        ├──img_2.png\n",
    "        └──img_1.png\n",
    "\n",
    "    \"\"\"\n",
    "    train_dataset = ImageFolder(IMAGE_DATA_DIR, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    # Initialize lists to store channel-wise means and standard deviations\n",
    "    channel_wise_means = [0.0, 0.0, 0.0]\n",
    "    channel_wise_stds = [0.0, 0.0, 0.0]\n",
    "\n",
    "    # Iterate through the training dataset to calculate means and standard deviations\n",
    "    for image, _ in train_dataset:\n",
    "        for i in range(3):  # Assuming RGB images\n",
    "            channel_wise_means[i] += image[i, :, :].mean().item()\n",
    "            channel_wise_stds[i] += image[i, :, :].std().item()\n",
    "\n",
    "    # Calculate the mean and standard deviation for each channel\n",
    "    num_samples = len(train_dataset)\n",
    "    channel_wise_means = [mean / num_samples for mean in channel_wise_means]\n",
    "    channel_wise_stds = [std / num_samples for std in channel_wise_stds]\n",
    "\n",
    "    # Print the mean and standard deviation for each channel\n",
    "    print(\"Mean:\", channel_wise_means)\n",
    "    print(\"Std:\", channel_wise_stds)\n",
    "\n",
    "    return channel_wise_means, channel_wise_stds\n",
    "\n",
    "# means, stds = get_mean_and_std_calculated(train_img_path_for_ImageFolder_dataloader)\n",
    "means = [0.44895144719250346, 0.4951483853617493, 0.4498602793532975]\n",
    "stds = [0.21388493326245522, 0.24571933703763144, 0.22413276759337405]\n",
    "\n",
    "normalize_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=means, std=stds) # always normalize only after tensor conversion\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSegmentationDataset(img_paths=train_img_path, mask_paths=train_mask_path, size=img_size, mode='binary', normalize=normalize_transform, transformations=None)\n",
    "val_dataset = SemanticSegmentationDataset(img_paths=val_img_path, mask_paths=val_mask_path, size=img_size, mode='binary', normalize=normalize_transform, transformations=None)\n",
    "test_dataset = SemanticSegmentationDataset(img_paths=val_img_path, mask_paths=None, size=img_size, normalize=normalize_transform, transformations=None)\n",
    "\n",
    "temp = train_dataset.__getitem__(1)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory  = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = 2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['batch_size'],\n",
    "    shuffle     = False,\n",
    "    drop_last   = False,\n",
    "    num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0622,  1.4280, -0.6979,  ...,  2.4224,  2.4695,  0.3204],\n",
      "          [ 2.1890,  2.9161,  3.5348,  ...,  1.3363,  2.0412, -0.1288],\n",
      "          [ 2.3432,  1.6509,  1.6003,  ..., -2.1547, -0.5840,  2.6106],\n",
      "          ...,\n",
      "          [ 4.4475,  5.9496,  6.7059,  ...,  7.6961,  5.3557,  4.7004],\n",
      "          [ 3.9954,  4.8844,  5.5944,  ...,  5.1450, -0.0916,  2.5199],\n",
      "          [ 3.2162,  3.3522,  3.5893,  ...,  3.7011,  0.7092,  0.3289]]]])\n"
     ]
    }
   ],
   "source": [
    "model = Unet(\n",
    "    # backbone='convnext_base', # backbone network name\n",
    "    backbone='resnet50',\n",
    "    preprocessing=True,\n",
    "    in_channels=3, # input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    num_classes=config[\"num_classes\"],  # output channels (number of classes in your dataset)\n",
    "    encoder_freeze=True,\n",
    "    pretrained=True,\n",
    ")\n",
    "\n",
    "# model = model().to(device)\n",
    "random_tensor = torch.rand((1, 3, 1024, 1024))\n",
    "print(model.predict(random_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msushantj\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/sush/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/wandb/run-20231116_213057-m12hnexj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sushantj/IDL_Project_Segmentation/runs/m12hnexj' target=\"_blank\">UNet_with_resnet_50</a></strong> to <a href='https://wandb.ai/sushantj/IDL_Project_Segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sushantj/IDL_Project_Segmentation' target=\"_blank\">https://wandb.ai/sushantj/IDL_Project_Segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sushantj/IDL_Project_Segmentation/runs/m12hnexj' target=\"_blank\">https://wandb.ai/sushantj/IDL_Project_Segmentation/runs/m12hnexj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define wandb credentials\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"49efd84d0e342f343fb91401332234dea4a3ffe2\") #API Key is in your wandb account, under settings (wandb.ai/settings)\n",
    "\n",
    "run = wandb.init(\n",
    "    name = \"UNet_with_resnet_50\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"IDL_Project_Segmentation\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/sush/klab2/Segmentation_Models/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|\u001b[32m██████████\u001b[0m| 98/98 [00:28<00:00,  3.48 training-batch/s, loss=0.107]\n",
      "Validation: 100%|\u001b[32m██████████\u001b[0m| 98/98 [00:09<00:00, 10.69 validating-batch/s, loss=nan]\n",
      "Traning Model on 10 epochs:  10%|█         | 1/10 [00:37<05:36, 37.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|\u001b[32m██████████\u001b[0m| 98/98 [00:30<00:00,  3.23 training-batch/s, loss=0.0903]\n",
      "Validation: 100%|\u001b[32m██████████\u001b[0m| 98/98 [00:09<00:00, 10.38 validating-batch/s, loss=nan]\n",
      "Traning Model on 10 epochs:  20%|██        | 2/10 [01:17<05:10, 38.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   7%|\u001b[32m▋         \u001b[0m| 7/98 [00:02<00:34,  2.62 training-batch/s, loss=0.0908]\n",
      "Traning Model on 10 epochs:  20%|██        | 2/10 [01:19<05:19, 39.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m mixed_precision_scaler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mGradScaler()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,              \u001b[39m# UNet model with Resnet50 backbone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     criterion\u001b[39m=\u001b[39mDiceLoss(),     \u001b[39m# loss function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39mcheckpoint_path\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sush/klab2/Segmentation_Models/pretrained-backbones-unet/unet_with_res50_segmentation.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_loader, val_loader)\n",
      "File \u001b[0;32m~/klab2/Segmentation_Models/pretrained-backbones-unet/backbones_unet/utils/trainer.py:76\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m# ---- train process ----\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m trange(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraning Model on \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m epochs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs)):\n\u001b[1;32m     75\u001b[0m     \u001b[39m# train\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_one_epoch(train_loader, epoch)\n\u001b[1;32m     77\u001b[0m     \u001b[39m# validate\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluate(val_loader, epoch)\n",
      "File \u001b[0;32m~/klab2/Segmentation_Models/pretrained-backbones-unet/backbones_unet/utils/trainer.py:143\u001b[0m, in \u001b[0;36mTrainer._train_one_epoch\u001b[0;34m(self, data_loader, epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    142\u001b[0m     training\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m--> 143\u001b[0m     losses[i] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_losses_[epoch \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=config['lr'], betas=(0.9, 0.999), weight_decay=0.05)\n",
    "gamma = 0.8\n",
    "milestones = [10,20,40,60,80]\n",
    "\n",
    "# scheduler1 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.9, total_iters=5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "# scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[20, 51])\n",
    "\n",
    "mixed_precision_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,              # UNet model with Resnet50 backbone\n",
    "    criterion=DiceLoss(),     # loss function\n",
    "    optimizer=optimizer,\n",
    "    epochs=10,\n",
    "    # scaler=mixed_precision_scaler,\n",
    "    lr_scheduler=scheduler,\n",
    "    device=device,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "\n",
    "trainer.fit(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
