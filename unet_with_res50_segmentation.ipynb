{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet with Resnet 50 Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Dataset Size: \n",
    "\n",
    "Val Dataset Size: \n",
    "\n",
    "Test Dataset Size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==1.13.1\n",
    "%pip install timm==0.6.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbones_unet.model.unet import Unet\n",
    "from backbones_unet.utils.dataset import SemanticSegmentationDataset\n",
    "from backbones_unet.model.losses import DiceLoss\n",
    "from backbones_unet.utils.trainer import Trainer\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from convert_coco_ann_to_mask import convert_coco_to_mask\n",
    "\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Installation\n",
    "random_tensor = torch.rand((1, 3, 64, 64))\n",
    "model = Unet(in_channels=3, num_classes=1) # if no backbone specified, will default to Resnet50\n",
    "print(model.predict(random_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add more items here\n",
    "config = {\n",
    "    \"lr\"         : 2e-3,\n",
    "    \"epochs\"     : 100,\n",
    "    \"batch_size\" : 2,  # Increase if your device can handle it\n",
    "    \"num_classes\": 1,\n",
    "    'truncated_normal_mean' : 0,\n",
    "    'truncated_normal_std' : 0.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a torch.utils.data.Dataset/DataLoader\n",
    "annotation_json_path = 'example_data/train/images'\n",
    "train_img_path = 'example_data/train/images'\n",
    "train_mask_path = 'example_data/train/masks'\n",
    "\n",
    "val_img_path = 'example_data/val/images'\n",
    "val_mask_path = 'example_data/val/masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Masks from the COCO annotations (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_coco_to_mask(input_json=annotation_json_path, image_folder=train_img_path, output_folder=train_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSegmentationDataset(train_img_path, train_mask_path)\n",
    "val_dataset = SemanticSegmentationDataset(val_img_path, val_mask_path)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    backbone='convnext_base', # backbone network name\n",
    "    in_channels=3,            # input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    num_classes=config[\"num_classes\"],            # output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wandb credentials\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"49efd84d0e342f343fb91401332234dea4a3ffe2\") #API Key is in your wandb account, under settings (wandb.ai/settings)\n",
    "\n",
    "run = wandb.init(\n",
    "    name = \"Trial_1\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"IDL_Project_Segmentation\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, 1e-4)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,                    # UNet model with pretrained backbone\n",
    "    criterion=DiceLoss(),     # loss function for model convergence\n",
    "    optimizer=optimizer,      # optimizer for regularization\n",
    "    epochs=10                 # number of epochs for model training\n",
    ")\n",
    "\n",
    "trainer.fit(train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDL_env_jupyter",
   "language": "python",
   "name": "idl_env_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
